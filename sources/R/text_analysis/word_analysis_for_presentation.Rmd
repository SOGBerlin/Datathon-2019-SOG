---
title: "DSSG 2019 - SOG Text Analysis"
author: "Eva Martinez"
output: 
  html_notebook:
    toc: true
---

```{r}
library(readr)
library(glue)
library(tidyverse) # to load tidyr, dplyr, ggplot2,...
# Text Mining ----------------------------------------
library(lsa) # stopwords in german
library(SnowballC) # word trunkating
library(tidytext) # text mining
# Visualization --------------------------------------
library(igraph) # networks viz
#library(wordcloud) # wordcloud viz
library(widyr)


data_categorized <- read_delim("data_categorized.csv", 
    ";", escape_double = FALSE, trim_ws = TRUE) %>% rowid_to_column()
#View(data_categorized)

questions_for_analysis <- read_csv("questions_for_analysis.csv")
```

This notebook generates the pdf's and graphs used for the final presentation.

```{r}
data(stopwords_de)

stopwords_de <- data_frame(token = stopwords_de)
```

```{r}
#' Tokenizes the text in a data frame removing numbers, signs and stopwords 
#'
#' @param df Input dataframe. 
#' @param curr_q dataframe with correspondence of current question numbers across the surveys.
#' @param custom_stopwords custom words to remove from dataframe.
#' @return Input dataframe with new colum "token" with 1 row per word.
tokenize_and_clean <- function(df = data_categorized, curr_q, custom_stopwords = NULL){
  data_categorized %>% 
    filter((Jahr == 2014 & Frage %in% curr_q$year_2014) | (Jahr == 2015 & Frage %in% curr_q$year_2015) | (Jahr == 2017 & Frage %in% curr_q$year_2017)) %>% # filter current question(s)
    unnest_tokens(token, Text) %>% # tokenize
    filter(str_detect(token, "[a-z]")) %>% # keep only words, remove numbers and signs 
    anti_join(stopwords_de, by = "token") %>% # remove stopwords
    filter(!(token %in% c("sog", "esf", "isea", "ja", "damit", "nein", "denn", "er", "projekt", custom_stopwords)) ) # remove custom stopwords
}

#' Generates Barplot of most common words.
#'
#' @param df Input (tokenized) dataframe. 
#' @param curr_q dataframe with correspondence of current question numbers across the surveys and labels for printing titles.
#' @return Barplot
count_words_ungrouped <- function(df, curr_q){
  clean.df %>%
    count(token, sort = TRUE) %>%
    filter(n > 1) %>% 
    top_n(n = 9, wt= n) %>% 
    mutate(token = reorder(token, n)) %>%
    ggplot(aes(token, n)) +
    theme_light() +
    geom_col(show.legend = FALSE) +
    coord_flip() + ggtitle(unique(curr_q$Label), subtitle = "Top 10 words")
}

#' Generates Barplot of most common words BY GENDER.
#'
#' @param df Input (tokenized) dataframe. 
#' @param curr_q dataframe with correspondence of current question numbers across the surveys and labels for printing titles.
#' @return Barplot
count_words_group_gender <- function(df, curr_q){
  df %>%
  filter(Geschlecht %in% c("männlich", "weiblich")) %>% 
  group_by(Geschlecht) %>%
  count(token, sort = TRUE) %>%
  filter(n > 1) %>% 
  top_n(n = 9, wt= n) %>% 
  ungroup() %>%
  mutate(token = reorder(token, n)) %>%
  ggplot(aes(token, n, fill = Geschlecht)) +
  theme_light() +
  geom_col(show.legend = FALSE) +
  facet_grid( ~ Geschlecht) +
  coord_flip() + ggtitle(unique(curr_q$Label), subtitle = "Top 10 words by Gender")
}

#' Generates Barplot of most common words BY YEAR
#'
#' @param df Input (tokenized) dataframe. 
#' @param curr_q dataframe with correspondence of current question numbers across the surveys and labels for printing titles.
#' @return Barplot
count_words_group_year <- function(df, curr_q){
  df %>%
  group_by(Jahr) %>%
  count(token, sort = TRUE) %>%
  filter(n > 1) %>% 
  top_n(n = 9, wt= n) %>% 
  ungroup() %>%
  mutate(token = reorder(token, n)) %>%
  ggplot(aes(token, n, fill = Jahr)) +
  theme_light() +
  geom_col(show.legend = FALSE) +
  facet_grid( ~ Jahr) +
  coord_flip() + ggtitle(unique(curr_q$Label), subtitle = "Top 10 words over year")
}

#' Generates Network words used over some threshold with following specs:
#' Node size = how much the words was used.
#' Edge size = how many times these words have been used together.
#'
#' @param df Input (tokenized) dataframe. 
#' @param threshold threshold fo display words with count > threshold
#' @param curr_q dataframe with correspondence of current question numbers across the surveys and labels for printing titles.
#' @param edge_w factor to scale edge width
#' @param vertex_s factor to scale vertex size
#' @return Network
network_generate <- function(df = clean.df, threshold, curr_q, edge_w = 3, vertex_s = 1/3, vertex_label_dist = 0.5, vertex_label_cex = 0.3){
  # We save the network plot as a pdf file.
  pdf(paste("exported_nw", unique(curr_q$Label), paste(unique(df$Jahr), collapse = "_"), ".pdf"))
  # Compute pairwise word count. 
  pair_wise_count_df <- df %>% 
  select(token, rowid) %>%
  pairwise_count(item = token, feature = rowid) %>% 
  filter(item1 < item2) %>% arrange(item1, item2) %>% 
  arrange(- n)
  # Define network object. 
  network <-  pair_wise_count_df %>%
    filter(n > threshold) %>% # apply threshold
    graph_from_data_frame(directed = FALSE)
  
  # Store the degree.
  V(network)$degree <- degree(graph = network)
  
  # Compute the weight shares.
  E(network)$width <- E(network)$n/max(E(network)$n)
   
  # Plot the network.
  plot(network, 
       vertex.color = 'lightblue',
       # Scale node size by degree.
       vertex.size = vertex_s*V(network)$degree,
       vertex.label.color = 'black', 
       vertex.label.cex = vertex_label_cex, 
       vertex.label.dist = vertex_label_dist,
       edge.color = 'gray', 
       # Set edge width proportional to the weight relative value.
       edge.width = edge_w*E(network)$width,
       main =  paste(unique(curr_q$Label), "for suveys:", paste(unique(df$Jahr), collapse = ",")), 
       sub = glue('Weight Threshold: {threshold}'), 
       alpha = 50)
  dev.off()
}
```

# SOG Improvements

```{r}
curr_q_num <- 1
curr_q <- questions_for_analysis %>% filter(ID == curr_q_num)
clean.df <- tokenize_and_clean(data_categorized, curr_q, custom_stopwords = c("gibt", "institution", "projekt", "stipendienprogramm", "programm", "stipendiaten"))
```

```{r}
count_words_ungrouped(clean.df, curr_q)
```

```{r}
count_words_group_gender(clean.df, curr_q)
```

```{r}
count_words_group_year(clean.df, curr_q)
```


```{r, fig.width=20}
# Set a threshold for visualization.
threshold <- 1

network_generate(df = clean.df, threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2014), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2015), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 0

#network_generate(df = filter(clean.df, Jahr == 2017), threshold, curr_q)
```


# Difficulties in the past academic year

```{r}
curr_q_num <- 2
curr_q <- questions_for_analysis %>% filter(ID == curr_q_num)
clean.df <- tokenize_and_clean(data_categorized, curr_q, custom_stopwords = c("schwierigkeiten"))
```

```{r}
count_words_ungrouped(clean.df, curr_q)
```

```{r}
count_words_group_gender(clean.df, curr_q)
```

```{r}
count_words_group_year(clean.df, curr_q)
```

```{r, fig.width=20}
# Set a threshold for visualization.
threshold <- 1

network_generate(df = clean.df, threshold, curr_q, vertex_s = 0.8, vertex_label_cex = 0.9, vertex_label_dist = 0.9)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2014), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2015), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 0

#network_generate(df = filter(clean.df, Jahr == 2017), threshold, curr_q)
```




# Possible risks of project implementation

```{r}
curr_q_num <- 3
curr_q <- questions_for_analysis %>% filter(ID == curr_q_num)
clean.df <- tokenize_and_clean(data_categorized, curr_q, custom_stopwords = c("größte", "größten", "all"))
```

```{r}
count_words_ungrouped(clean.df, curr_q)
```

```{r}
count_words_group_gender(clean.df, curr_q)
```

```{r}
count_words_group_year(clean.df, curr_q)
```

```{r, fig.width=20}
# Set a threshold for visualization.
threshold <- 1

network_generate(df = clean.df, threshold, curr_q, vertex_s = 0.4, vertex_label_cex = 0.7, vertex_label_dist = 0.8)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2014), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2015), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 0

#network_generate(df = filter(clean.df, Jahr == 2017), threshold, curr_q)
```




# Changes in Life Situation

```{r}
curr_q_num <- 5
curr_q <- questions_for_analysis %>% filter(ID == curr_q_num)
clean.df <- tokenize_and_clean(data_categorized, curr_q, custom_stopwords = c("situation", "verändert", "veränderung", "jahr", "geändert", "gibt"))
```

```{r}
count_words_ungrouped(clean.df, curr_q)
```

```{r}
count_words_group_gender(clean.df, curr_q)
```

```{r}
count_words_group_year(clean.df, curr_q)
```

```{r, fig.width=20}
# Set a threshold for visualization.
threshold <- 1

network_generate(df = clean.df, threshold, curr_q, vertex_s = 0.8, vertex_label_cex = 1, vertex_label_dist = 0.9)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2014), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2015), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 0

#network_generate(df = filter(clean.df, Jahr == 2017), threshold, curr_q)
```



# Wishes for next year

```{r}
curr_q_num <- 6
curr_q <- questions_for_analysis %>% filter(ID == curr_q_num)
clean.df <- tokenize_and_clean(data_categorized, curr_q, custom_stopwords = c("jahr", "möchte", "nächtes", "nächstes", "wünsche", "wunsch", "wünscht", "hofft", "man"))
```

```{r}
count_words_ungrouped(clean.df, curr_q)
```

```{r}
count_words_group_gender(clean.df, curr_q)
```

```{r}
count_words_group_year(clean.df, curr_q)
```

```{r, fig.width=20}
# Set a threshold for visualization.
threshold <- 1

network_generate(df = clean.df, threshold, curr_q, vertex_s = 1, vertex_label_cex = 0.8, vertex_label_dist = 0.6)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2014), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 1

#network_generate(df = filter(clean.df, Jahr == 2015), threshold, curr_q)
```

```{r}
# Set a threshold for visualization.
#threshold <- 0

#network_generate(df = filter(clean.df, Jahr == 2017), threshold, curr_q)
```

